{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 950 (CNMeM is enabled with initial size: 66.0% of memory, cuDNN 5105)\n",
      "/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import collections\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531\n",
      "27629896\n"
     ]
    }
   ],
   "source": [
    "def remove_hidden(l):\n",
    "    return filter(lambda s: not s[0] == '.', l)\n",
    "\n",
    "d = \"//home//xgamer//OANC-GrAF//data//written_1//journal//slate\"\n",
    "\n",
    "def get_files(d):\n",
    "    files = os.listdir(d)\n",
    "    files = remove_hidden(files)\n",
    "    return map(lambda f: d + \"//\" + f, files)\n",
    "\n",
    "dirs = get_files(d)\n",
    "files = map(get_files, dirs)\n",
    "files = [f for l in files for f in l]\n",
    "files = filter(lambda f: f[-3:] == 'txt', files)\n",
    "\n",
    "print len(files)\n",
    "\n",
    "texts = []\n",
    "for f in files:\n",
    "    f = open(f)\n",
    "    texts.append('\\n'.join(f.readlines()[7:]))\n",
    "    f.close()\n",
    "\n",
    "print sum([len(text) for text in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25425786\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('--', ' - ')\n",
    "    text = text.replace('\\t', ' ')\n",
    "    text = re.sub(r'\\ \\ +', r' ', text)\n",
    "    text = filter(lambda char: char in string.printable, text)\n",
    "    return text\n",
    "\n",
    "texts = map(clean_text, texts)\n",
    "print sum([len(text) for text in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trustbusters Have Job Security The Dec. 14 edition of \"Today's Papers\" notes a Washington Post report that David Boies, the lead Justice Department attorney in the Microsoft case, has lowered the hourly fee that he is charging the government. The column asks whether this is legal, if Boies' motive was to keep his share of the work. The answer can be found in the definitive rhyming opus on antitrust law, R.W. Grant's Tom Smith and His Incredible Bread Machine : You're gouging on your prices if You charge more than the rest. But it's unfair competition If you think you can charge less! A second point that we would make To help avoid confusion Don't try to charge the same amount! That would be collusion! In short, Boies' reduction of his fee constitutes unfair competition. Of course, if he'd left his fee unchanged, that would be collusion, though in this case he'd be colluding with himself and would eventually go blind. How's that for legal clarity? - Sam Kazman Washington Bork and Mindy Are there two Robert Borks, as Michael Kinsley writes in \"\"? But not just on antitrust. We have two Borks for probably any subject. Give the man a retainer or the proper ideological cause and he readily will tailor his views to the needs of the moment. I interviewed Bork several years ago for my book The Wars of Watergate . His role in the Watergate affair has, in all fairness, been badly distorted. Never mind. In our conversation, he spoke very forcefully against the special prosecutor (now independent counsel) statute. \"You cannot bag a case in the Justice Department,\" he told me. \"Too many lawyers, who like to talk to too many reporters.\" He was absolutely right, of course. Fast forward to 1994 and beyond. An independent counsel to investigate President Clinton? Of course. And \"Judge\" Kenneth Starr to conduct the investigation? Of course, said Bork, who appears as a regular cast member in those situation comedies that pass as \"talk shows\" every night, vigorously extolling the virtues and honesty of \"Judge\" Starr. Kinsley comes close to the real reason for rejecting Bork for a seat on the Supreme Court. The man has the intellectual honesty of a hired gun. Which he has truly become. - Stanley I. Kutler Fox Professor of American Institutions University of Wisconsin Madison, Wis. First, We Kill All the Lawyers I'm sure Michael Kinsley's \"Book Bork, Browser Bork\" is right to say that Robert Bork's current posture on the Microsoft antitrust case represents a departure from his 20-year-old book on antitrust law. But since when is a lawyer expected to actually possess the views he espouses on behalf of his clients? If the Kinsley Doctrine is that a client can only select a lawyer who honestly believes in the legal arguments of the client, then Bill Clinton wouldn't have any lawyers. - Robert Little Memphis, Tenn. Perjury Is Bad, Smearing Is Worse Just a quick point on David Plotz's \"Dispatch \" from the Dec. 9 House Judiciary Committee hearing. Plotz writes, \"Yesterday I called Lindsey Graham, R-S.C., 'admirable.' I apologize. ... This afternoon, Graham reveals himself, and he does it in such a calculating way that I wonder whether everything he has done till now has been pure performance. As the final speaker on the final day of hearings, Graham unleashes a 15 minute harangue about Clintonian evil. Clinton's true crime is not perjury or adultery, Graham shouts, it's that he planned to destroy Monica.\" Isn't the most reprehensible aspect of Clinton's conduct from a moral point of view the fact that he either orchestrated or knowingly permitted the resources of his position and office to be used in an attempt to destroy the reputations of those who have accused him of misdeeds? The story of the sexual activity is disturbing, given the power imbalances between the participants, but the story is a tawdry one, and most well-mannered people would just as soon not have to hear about it, especially since the \"victim\" is not making any complaint. The lying can be viewed as a natural human failing, which most people, it appears, are ready to forgive. Lying under oath is a more serious matter, but it is more serious for institutional reasons rather than purely moral ones. As the head of state and a living symbol of a system, which, in the name of justice, imposes laws upon and claims the right to exercise coercive power over all Americans, the president has a special duty - higher than that of an ordinary citizen - to abide by the rules that the system imposes. One of the most important rules for the whole justice system is the penalty against perjury. It is only right that Congress should take that issue seriously. But from a purely moral point of view, is it not utterly despicable for the president of the United States to use the power of his office to destroy the reputation of a person for the sole purpose of ensuring that that person will not be believed when she tells the truth? Such conduct may not be a \"high crime or misdemeanor.\" It may not even be illegal. But it is surely a moral abuse of the powers of the presidency that should not go unnoticed. - Kenneth J. Tyler Vancouver, British Columbia Jocks of the World, Unite! You Have Nothing To Lose but Your Scholarships! Jim Naughton's \"\" has missed the forest for the trees. He complains that a few powerful schools are aligning themselves to take a larger slice of the revenues generated by big-time college athletics and bemoans the small colleges whose athletic programs will be destroyed by this. What Naughton has missed is that the entire NCAA system is corrupt, making millions of dollars off student athletes who can't even accept plane fare from alumni to visit family. That's the real travesty of college sports. Arguing which schools should keep the huge revenues generated by athletes really misses the point. - Anthony Lapadula Redmond, Wash. Address your e-mail to the editors to letters@slate.com. You must include your address and daytime phone number (for confirmation only).\n"
     ]
    }
   ],
   "source": [
    "print texts[545]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}', '~', '\\t', '\\n']\n",
      "{0: ' ', 1: '!', 2: '\"', 3: '#', 4: '$', 5: '%', 6: '&', 7: \"'\", 8: '(', 9: ')', 10: '*', 11: '+', 12: ',', 13: '-', 14: '.', 15: '/', 16: '0', 17: '1', 18: '2', 19: '3', 20: '4', 21: '5', 22: '6', 23: '7', 24: '8', 25: '9', 26: ':', 27: ';', 28: '<', 29: '=', 30: '>', 31: '?', 32: '@', 33: 'A', 34: 'B', 35: 'C', 36: 'D', 37: 'E', 38: 'F', 39: 'G', 40: 'H', 41: 'I', 42: 'J', 43: 'K', 44: 'L', 45: 'M', 46: 'N', 47: 'O', 48: 'P', 49: 'Q', 50: 'R', 51: 'S', 52: 'T', 53: 'U', 54: 'V', 55: 'W', 56: 'X', 57: 'Y', 58: 'Z', 59: '[', 60: '\\\\', 61: ']', 62: '^', 63: '_', 64: '`', 65: 'a', 66: 'b', 67: 'c', 68: 'd', 69: 'e', 70: 'f', 71: 'g', 72: 'h', 73: 'i', 74: 'j', 75: 'k', 76: 'l', 77: 'm', 78: 'n', 79: 'o', 80: 'p', 81: 'q', 82: 'r', 83: 's', 84: 't', 85: 'u', 86: 'v', 87: 'w', 88: 'x', 89: 'y', 90: 'z', 91: '{', 92: '}', 93: '~', 94: '\\t', 95: '\\n'}\n",
      "{' ': 0, '$': 4, '(': 8, ',': 12, '0': 16, '4': 20, '8': 24, '<': 28, '@': 32, 'D': 36, 'H': 40, 'L': 44, 'P': 48, 'T': 52, 'X': 56, '\\\\': 60, '`': 64, 'd': 68, 'h': 72, 'l': 76, 'p': 80, 't': 84, 'x': 88, '#': 3, \"'\": 7, '+': 11, '/': 15, '3': 19, '7': 23, ';': 27, '?': 31, 'C': 35, 'G': 39, 'K': 43, 'O': 47, 'S': 51, 'W': 55, '[': 59, '_': 63, 'c': 67, 'g': 71, 'k': 75, 'o': 79, 's': 83, 'w': 87, '{': 91, '\\n': 95, '\"': 2, '&': 6, '*': 10, '.': 14, '2': 18, '6': 22, ':': 26, '>': 30, 'B': 34, 'F': 38, 'J': 42, 'N': 46, 'R': 50, 'V': 54, 'Z': 58, '^': 62, 'b': 66, 'f': 70, 'j': 74, 'n': 78, 'r': 82, 'v': 86, 'z': 90, '~': 93, '\\t': 94, '!': 1, '%': 5, ')': 9, '-': 13, '1': 17, '5': 21, '9': 25, '=': 29, 'A': 33, 'E': 37, 'I': 41, 'M': 45, 'Q': 49, 'U': 53, 'Y': 57, ']': 61, 'a': 65, 'e': 69, 'i': 73, 'm': 77, 'q': 81, 'u': 85, 'y': 89, '}': 92}\n"
     ]
    }
   ],
   "source": [
    "alphabet = collections.defaultdict(bool)\n",
    "\n",
    "for text in texts:\n",
    "    for char in text:\n",
    "        alphabet[char] = True\n",
    "\n",
    "alphabet = [char for char in alphabet if alphabet[char] == True]\n",
    "alphabet.sort()\n",
    "\n",
    "# Add a start of sequence token \\t\n",
    "alphabet.append('\\t')\n",
    "# Add an end of sequence token \\n\n",
    "alphabet.append('\\n')\n",
    "\n",
    "texts = map(lambda text: '\\t' + text + '\\n', texts)\n",
    "print alphabet\n",
    "\n",
    "char_to_index = {}\n",
    "index_to_char = {}\n",
    "for i in xrange(len(alphabet)):\n",
    "    index_to_char[i] = alphabet[i]\n",
    "    char_to_index[alphabet[i]] = i\n",
    "print index_to_char\n",
    "print char_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " quotidian proximity that the role affords, has demonstrated a proven knack for understated aptness in its selections. Ronald \"Rawhide\" Reagan. Dan \"Scorecard\" Quayle. Edward \"Sunburn\" Kennedy. Jimmy \"\n",
      "[ 0 81 85 79 84 73 68 73 65 78  0 80 82 79 88 73 77 73 84 89  0 84 72 65 84\n",
      "  0 84 72 69  0 82 79 76 69  0 65 70 70 79 82 68 83 12  0 72 65 83  0 68 69\n",
      " 77 79 78 83 84 82 65 84 69 68  0 65  0 80 82 79 86 69 78  0 75 78 65 67 75\n",
      "  0 70 79 82  0 85 78 68 69 82 83 84 65 84 69 68  0 65 80 84 78 69 83 83  0\n",
      " 73 78  0 73 84 83  0 83 69 76 69 67 84 73 79 78 83 14  0 50 79 78 65 76 68\n",
      "  0  2 50 65 87 72 73 68 69  2  0 50 69 65 71 65 78 14  0 36 65 78  0  2 51\n",
      " 67 79 82 69 67 65 82 68  2  0 49 85 65 89 76 69 14  0 37 68 87 65 82 68  0\n",
      "  2 51 85 78 66 85 82 78  2  0 43 69 78 78 69 68 89 14  0 42 73 77 77 89  0]\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "200 200\n"
     ]
    }
   ],
   "source": [
    "text_seq = ''.join(texts)\n",
    "sample_len = 200\n",
    "\n",
    "def sample(text, length=sample_len + 1):\n",
    "    i = random.randint(0, len(text) - length - 1)\n",
    "    return text[i:i+length]\n",
    "\n",
    "def toseqs(string, alphabet=alphabet):\n",
    "    assert len(string) > 1\n",
    "    n = len(string) - 1\n",
    "    data = np.zeros(n, dtype='int')\n",
    "    label = np.zeros(len(alphabet), dtype='uint8')\n",
    "    for i in xrange(n):\n",
    "        cur = string[i]\n",
    "        data[i] = char_to_index[cur]\n",
    "    nxt = string[i+1]\n",
    "    label[char_to_index[nxt]] = 1\n",
    "    return data, label\n",
    "\n",
    "s = sample(text_seq)\n",
    "print s\n",
    "x,y = toseqs(s)\n",
    "print x\n",
    "print y\n",
    "\n",
    "def tostring(seq, alphabet=alphabet):\n",
    "    string = []\n",
    "    for i in xrange(seq.shape[0]):\n",
    "        string.append(index_to_char[np.argmax(seq[i])])\n",
    "    return ''.join(string)\n",
    "\n",
    "print len(x), len(tostring(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "NN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 96\n"
     ]
    }
   ],
   "source": [
    "embedding_size = len(alphabet)\n",
    "n_lstm_cells = 1000\n",
    "dropout_perc = 0.1\n",
    "\n",
    "model = Sequential()\n",
    "print sample_len, len(alphabet)\n",
    "model.add(Embedding(len(alphabet), embedding_size, input_length=sample_len))\n",
    "model.add(LSTM(n_lstm_cells))\n",
    "model.add(Dropout(dropout_perc))\n",
    "model.add(Dense(len(alphabet)))\n",
    "model.add(Activation('softmax'))\n",
    "#opt = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "opt = RMSprop()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "n_batches_per_epoch = 5001\n",
    "\n",
    "def get_batch(size, text=text_seq):\n",
    "    xs = np.zeros((size, sample_len), dtype='int')\n",
    "    ys = np.zeros((size, len(alphabet)), dtype='uint8')\n",
    "    for i in xrange(size):\n",
    "        s = sample(text)\n",
    "        xs[i],ys[i] = toseqs(s)\n",
    "    return xs, ys\n",
    "\n",
    "valid_x, valid_y = get_batch(batch_size)\n",
    "seen_samples = 0\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saw 326528 samples.\n",
      "Ran 3673.57299304 seconds. (This epoch: 94.2069131202 samples / second)\n",
      "Train accuracy: 0.490625\n",
      "Test accuracy: 0.59375\n",
      "\n",
      "Saw 332928 samples.\n",
      "Ran 3742.15835118 seconds. (This epoch: 93.7607414666 samples / second)\n",
      "Train accuracy: 0.50125\n",
      "Test accuracy: 0.53125\n",
      "\n",
      "Saw 339328 samples.\n",
      "Ran 3810.96644211 seconds. (This epoch: 93.5107631586 samples / second)\n",
      "Train accuracy: 0.4871875\n",
      "Test accuracy: 0.5625\n",
      "\n",
      "Saw 345728 samples.\n",
      "Ran 3879.58535719 seconds. (This epoch: 93.4502916863 samples / second)\n",
      "Train accuracy: 0.49578125\n",
      "Test accuracy: 0.53125\n",
      "\n",
      "Saw 352128 samples.\n",
      "Ran 3947.83170414 seconds. (This epoch: 93.5155022151 samples / second)\n",
      "Train accuracy: 0.48125\n",
      "Test accuracy: 0.546875\n",
      "\n",
      "Saw 358528 samples.\n",
      "Ran 4016.30265307 seconds. (This epoch: 93.50797768 samples / second)\n",
      "Train accuracy: 0.4875\n",
      "Test accuracy: 0.578125\n",
      "\n",
      "Saw 364928 samples.\n",
      "Ran 4084.93296719 seconds. (This epoch: 93.4715551915 samples / second)\n",
      "Train accuracy: 0.48984375\n",
      "Test accuracy: 0.59375\n",
      "\n",
      "Saw 371328 samples.\n",
      "Ran 4156.81802416 seconds. (This epoch: 92.8931329516 samples / second)\n",
      "Train accuracy: 0.5003125\n",
      "Test accuracy: 0.53125\n",
      "\n",
      "Saw 377728 samples.\n",
      "Ran 4226.26196122 seconds. (This epoch: 92.8112649976 samples / second)\n",
      "Train accuracy: 0.4971875\n",
      "Test accuracy: 0.609375\n",
      "\n",
      "Saw 384128 samples.\n",
      "Ran 4296.36841702 seconds. (This epoch: 92.6569878043 samples / second)\n",
      "Train accuracy: 0.4934375\n",
      "Test accuracy: 0.59375\n",
      "\n",
      "Saw 390528 samples.\n",
      "Ran 4367.13734317 seconds. (This epoch: 92.4506905035 samples / second)\n",
      "Train accuracy: 0.50265625\n",
      "Test accuracy: 0.59375\n",
      "\n",
      "Saw 396928 samples.\n",
      "Ran 4436.681458 seconds. (This epoch: 92.4153404859 samples / second)\n",
      "Train accuracy: 0.515\n",
      "Test accuracy: 0.578125\n",
      "\n",
      "Saw 403328 samples.\n",
      "Ran 4507.0780251 seconds. (This epoch: 92.2981468492 samples / second)\n",
      "Train accuracy: 0.50328125\n",
      "Test accuracy: 0.59375\n",
      "\n",
      "Saw 409728 samples.\n",
      "Ran 4577.32620811 seconds. (This epoch: 92.2119890368 samples / second)\n",
      "Train accuracy: 0.5046875\n",
      "Test accuracy: 0.5625\n",
      "\n",
      "Saw 416128 samples.\n",
      "Ran 4648.4445622 seconds. (This epoch: 92.0606072142 samples / second)\n",
      "Train accuracy: 0.5065625\n",
      "Test accuracy: 0.53125\n",
      "\n",
      "Saw 422528 samples.\n",
      "Ran 4719.7681222 seconds. (This epoch: 91.9116219749 samples / second)\n",
      "Train accuracy: 0.50546875\n",
      "Test accuracy: 0.546875\n",
      "\n",
      "Saw 428928 samples.\n",
      "Ran 4792.80206418 seconds. (This epoch: 91.6483988002 samples / second)\n",
      "Train accuracy: 0.503125\n",
      "Test accuracy: 0.546875\n",
      "\n",
      "Saw 435328 samples.\n",
      "Ran 4865.8122611 seconds. (This epoch: 91.4173906103 samples / second)\n",
      "Train accuracy: 0.52171875\n",
      "Test accuracy: 0.625\n",
      "\n",
      "Saw 441728 samples.\n",
      "Ran 4939.53048801 seconds. (This epoch: 91.1632807994 samples / second)\n",
      "Train accuracy: 0.51484375\n",
      "Test accuracy: 0.59375\n",
      "\n",
      "Saw 448128 samples.\n",
      "Ran 5012.24877214 seconds. (This epoch: 91.0003881183 samples / second)\n",
      "Train accuracy: 0.5040625\n",
      "Test accuracy: 0.546875\n",
      "\n",
      "Saw 454528 samples.\n",
      "Ran 5084.63549018 seconds. (This epoch: 90.8738609217 samples / second)\n",
      "Train accuracy: 0.51421875\n",
      "Test accuracy: 0.546875\n",
      "\n",
      "Saw 460928 samples.\n",
      "Ran 5158.40137005 seconds. (This epoch: 90.6785590988 samples / second)\n",
      "Train accuracy: 0.51046875\n",
      "Test accuracy: 0.5625\n",
      "\n",
      "Saw 467328 samples.\n",
      "Ran 5231.582021 seconds. (This epoch: 90.5335249734 samples / second)\n",
      "Train accuracy: 0.5053125\n",
      "Test accuracy: 0.53125\n",
      "\n",
      "Saw 473728 samples.\n",
      "Ran 5304.22593212 seconds. (This epoch: 90.4295335157 samples / second)\n",
      "Train accuracy: 0.5265625\n",
      "Test accuracy: 0.578125\n",
      "\n",
      "Saw 480128 samples.\n",
      "Ran 5376.8429122 seconds. (This epoch: 90.3354418289 samples / second)\n",
      "Train accuracy: 0.52421875\n",
      "Test accuracy: 0.5625\n",
      "\n",
      "Saw 486528 samples.\n",
      "Ran 5449.80092001 seconds. (This epoch: 90.232075913 samples / second)\n",
      "Train accuracy: 0.51\n",
      "Test accuracy: 0.609375\n",
      "\n",
      "Saw 492928 samples.\n",
      "Ran 5522.68336415 seconds. (This epoch: 90.1401262301 samples / second)\n",
      "Train accuracy: 0.5153125\n",
      "Test accuracy: 0.609375\n",
      "\n",
      "Saw 499328 samples.\n",
      "Ran 5596.13305211 seconds. (This epoch: 90.0292549374 samples / second)\n",
      "Train accuracy: 0.518125\n",
      "Test accuracy: 0.5625\n",
      "\n",
      "Saw 505728 samples.\n",
      "Ran 5668.97410417 seconds. (This epoch: 89.9527896179 samples / second)\n",
      "Train accuracy: 0.51984375\n",
      "Test accuracy: 0.609375\n",
      "\n",
      "Saw 512128 samples.\n",
      "Ran 5741.76467013 seconds. (This epoch: 89.8836610095 samples / second)\n",
      "Train accuracy: 0.5190625\n",
      "Test accuracy: 0.5625\n",
      "\n",
      "Saw 518528 samples.\n",
      "Ran 5815.07811618 seconds. (This epoch: 89.7978371955 samples / second)\n",
      "Train accuracy: 0.5271875\n",
      "Test accuracy: 0.546875\n",
      "\n",
      "Saw 524928 samples.\n",
      "Ran 5888.686836 seconds. (This epoch: 89.7059244244 samples / second)\n",
      "Train accuracy: 0.5259375\n",
      "Test accuracy: 0.625\n",
      "\n",
      "Saw 531328 samples.\n",
      "Ran 5961.57579303 seconds. (This epoch: 89.6471235129 samples / second)\n",
      "Train accuracy: 0.52296875\n",
      "Test accuracy: 0.625\n",
      "\n",
      "Saw 537728 samples.\n",
      "Ran 6033.89869618 seconds. (This epoch: 89.6127298225 samples / second)\n",
      "Train accuracy: 0.52375\n",
      "Test accuracy: 0.609375\n",
      "\n",
      "Saw 544128 samples.\n",
      "Ran 6106.02050519 seconds. (This epoch: 89.5875276588 samples / second)\n",
      "Train accuracy: 0.52390625\n",
      "Test accuracy: 0.59375\n",
      "\n",
      "Saw 550528 samples.\n",
      "Ran 6178.59540606 seconds. (This epoch: 89.5479704387 samples / second)\n",
      "Train accuracy: 0.5325\n",
      "Test accuracy: 0.6875\n",
      "\n",
      "Saw 556928 samples.\n",
      "Ran 6252.07368517 seconds. (This epoch: 89.480035829 samples / second)\n",
      "Train accuracy: 0.51921875\n",
      "Test accuracy: 0.640625\n",
      "\n",
      "Saw 563328 samples.\n",
      "Ran 6325.54976106 seconds. (This epoch: 89.415843211 samples / second)\n",
      "Train accuracy: 0.52515625\n",
      "Test accuracy: 0.625\n",
      "\n",
      "Saw 569728 samples.\n",
      "Ran 6398.22738719 seconds. (This epoch: 89.380568695 samples / second)\n",
      "Train accuracy: 0.52140625\n",
      "Test accuracy: 0.65625\n",
      "\n",
      "Saw 576128 samples.\n",
      "Ran 6470.53649306 seconds. (This epoch: 89.3585734831 samples / second)\n",
      "Train accuracy: 0.5159375\n",
      "Test accuracy: 0.6875\n",
      "\n",
      "Saw 582528 samples.\n",
      "Ran 6543.09029007 seconds. (This epoch: 89.3302208444 samples / second)\n",
      "Train accuracy: 0.5184375\n",
      "Test accuracy: 0.671875\n",
      "\n",
      "Saw 588928 samples.\n",
      "Ran 6616.05861211 seconds. (This epoch: 89.290940747 samples / second)\n",
      "Train accuracy: 0.5259375\n",
      "Test accuracy: 0.640625\n",
      "\n",
      "Saw 595328 samples.\n",
      "Ran 6688.85059714 seconds. (This epoch: 89.2586228819 samples / second)\n",
      "Train accuracy: 0.51875\n",
      "Test accuracy: 0.703125\n",
      "\n",
      "Saw 601728 samples.\n",
      "Ran 6761.31512308 seconds. (This epoch: 89.2370525108 samples / second)\n",
      "Train accuracy: 0.5271875\n",
      "Test accuracy: 0.71875\n",
      "\n",
      "Saw 608128 samples.\n",
      "Ran 6834.15198803 seconds. (This epoch: 89.2061633339 samples / second)\n",
      "Train accuracy: 0.51671875\n",
      "Test accuracy: 0.734375\n",
      "\n",
      "Saw 614528 samples.\n",
      "Ran 6906.42621207 seconds. (This epoch: 89.191834486 samples / second)\n",
      "Train accuracy: 0.52015625\n",
      "Test accuracy: 0.625\n",
      "\n",
      "Saw 620928 samples.\n",
      "Ran 6978.61124921 seconds. (This epoch: 89.1804770991 samples / second)\n",
      "Train accuracy: 0.5265625\n",
      "Test accuracy: 0.65625\n",
      "\n",
      "Saw 627328 samples.\n",
      "Ran 7051.07528305 seconds. (This epoch: 89.1623764058 samples / second)\n",
      "Train accuracy: 0.53015625\n",
      "Test accuracy: 0.671875\n",
      "\n",
      "Saw 633728 samples.\n",
      "Ran 7124.24005103 seconds. (This epoch: 89.1272713523 samples / second)\n",
      "Train accuracy: 0.53203125\n",
      "Test accuracy: 0.640625\n",
      "\n",
      "Saw 640128 samples.\n",
      "Ran 7196.6014781 seconds. (This epoch: 89.1135237331 samples / second)\n",
      "Train accuracy: 0.52875\n",
      "Test accuracy: 0.671875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mu_acc = 0.0\n",
    "e_start = time.time()\n",
    "start_samples = seen_samples\n",
    "output_every_n = 100\n",
    "\n",
    "for i in xrange(n_batches_per_epoch):\n",
    "    x,y = get_batch(batch_size)\n",
    "    seen_samples += batch_size\n",
    "    _,acc =  model.train_on_batch(x, y)\n",
    "    mu_acc += acc\n",
    "    if i % output_every_n == 0 and i > 0:\n",
    "        print \"Saw \" + str(seen_samples) + \" samples.\"\n",
    "        t = time.time()\n",
    "        print \"Ran \" + str(t-start) + \" seconds. (This epoch: \" +\\\n",
    "            str((seen_samples-start_samples)*1./(t-e_start)) + \" samples / second)\"\n",
    "        print \"Train accuracy: \" + str(1./float(output_every_n)*mu_acc)\n",
    "        mu_acc = 0.0\n",
    "        _,acc = model.test_on_batch(valid_x, valid_y)\n",
    "        print \"Test accuracy: \" + str(acc)\n",
    "        print\n",
    "\n",
    "model.save(\"charlevel_b2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBill and Hillary Clinton could dragn the president that the president is a lottered and reports and someone of the and of the prission of the court to get it was a companies and behind the but one of the some way pronection in the anticle in China that we said the start to take the country, his last most beungress of a smarter and in convention of the committed mention of a resuarches to the last term of the inventional design in the can be a companious morning to the simple of the New York Times Backs with the states t\n",
      "\tBill and Hillary Clinton said. The country. The secret and self-corperction to friend that the congressional prompity of the last describes are noted a fair of the policy called the resson that the years ago, which centrally religing the books not be a time which are disingents to the president of the way the boster to be an as an a high-states of problems a beautic religious as which friends and every broke as a sping of the discussion to be a to allowed the work for marriage than access in the world to casing the new \n",
      "\tBill and Hillary Clinton believed the same than has been experient that the had to be a companious story or elock to the Hister. So being the column and a lost thing of the state being the fast the religious click that the president be and continues and a big on the books to be a reservist mention to the feet of the last all the new was a companies in Allin has a concerned than the states of a book that concerned in the magazine and the teacher than connect of the same experic to concerned that perchange to controver hi\n",
      "\tBill and Hillary Clinton could never free in the most protect of the fact to concent than the some course, which the president in the court release in the story can a provide the secretary and the advice from a point as the believer in a commonity to religious interest with a money was be someone of the half says the comes could not his spend to co say the post of secies that an internet could say the sension of the demonstrages in the story are beyond the story pointed to the political country of the story passes he di\n",
      "\tBill and Hillary Clinton consider the companies with the resumit and specific critic profection. But the felt and magazines on the result and increase the exis with a presidence to inventing the other national particulate schiles that the crustic high-strights and service the attend Against action to make it is all the resign of friend and before the raping but the ware that the book made a moving the leage that the current of the time, and the bracking profection to companies the some advice of site of the solitical pr\n"
     ]
    }
   ],
   "source": [
    "text_len = 500\n",
    "n_samples = 5\n",
    "seed_string = \"\\tBill and Hillary Clinton \"\n",
    "\n",
    "def tochar_prob(output):\n",
    "    summed = []\n",
    "    _sum = 0.0\n",
    "    output = output[0]\n",
    "    output = output**2\n",
    "    output /= sum(output)\n",
    "    for i in xrange(output.shape[0]):\n",
    "        _sum += output[i]\n",
    "        summed.append(_sum)\n",
    "    choice = random.random()\n",
    "    i = bisect.bisect(summed, choice)\n",
    "    return index_to_char[i]\n",
    "        \n",
    "def generate(length, seed='\\t', model=model):\n",
    "    seq = np.zeros((1, sample_len))\n",
    "    padding_len = sample_len - len(seed)\n",
    "    if padding_len < 1:\n",
    "        padding_len = 0\n",
    "    for i in xrange(padding_len):\n",
    "        seq[0,i] = 0\n",
    "    for i in xrange(len(seed)):\n",
    "        seq[0,padding_len+i] = char_to_index[seed[i]]\n",
    "        \n",
    "    string = list(seed_string)\n",
    "    for i in xrange(length):\n",
    "        c = tochar_prob(model.predict(seq))\n",
    "        string.append(c)\n",
    "        if c == '\\n':\n",
    "            break\n",
    "        seq = np.roll(seq, -1, axis=1)\n",
    "        seq[0,-1] = char_to_index[c]\n",
    "    return ''.join(string)\n",
    "\n",
    "for i in xrange(n_samples):\n",
    "    print generate(text_len, seed_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
