{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just various neural networks and associated techniques in a modular fashion utilizing matrix notation.\n",
    "# Nothing groundbreaking; just for exercising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SIGMA_INIT = 0.01\n",
    "\n",
    "class Activation(object):\n",
    "    def __init__(self, f, f_deriv):\n",
    "        self.f = f\n",
    "        self.f_deriv = f_deriv\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self, n, activation, bias):\n",
    "        self.n = n\n",
    "        self.W = None\n",
    "        self.act = activation\n",
    "        self.bias = bias\n",
    "\n",
    "class Loss(object):\n",
    "    def __init__(self, loss_function, loss_function_deriv):\n",
    "        self.f = loss_function\n",
    "        self.f_deriv = loss_function_deriv\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, learning_rate):\n",
    "        self.lr = learning_rate\n",
    "    def update(self):\n",
    "        raise Exception(\"Use implementing subclass.\")\n",
    "\n",
    "class SGD(Trainer):\n",
    "    def __init__(self, learning_rate, momentum):\n",
    "        super(SGD, self).__init__(learning_rate)\n",
    "        self.momentum = momentum\n",
    "    def update(self):\n",
    "        for i, layer in enumerate(self.model.layers):\n",
    "            if self.deltas[i] != None:\n",
    "                layer.W -= self.momentum * self.deltas[i]\n",
    "            # consider bias\n",
    "            imax = layer.W.shape[0]\n",
    "            delta = self.lr * np.dot(layer.deltas[:imax], layer._in.T) / layer.deltas.shape[1]\n",
    "            layer.W -= delta\n",
    "            self.deltas[i] = delta\n",
    "        \n",
    "# TODO: Change to in place operations for optimization where possible.\n",
    "class Model(object):\n",
    "    def __init__(self, n_in):\n",
    "        self.n_in = n_in\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.trainer = None\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    def prepare(self, loss, trainer):\n",
    "        self.nl = len(self.layers)\n",
    "        assert self.nl > 0\n",
    "        n_in = self.n_in\n",
    "        for i in xrange(self.nl):\n",
    "            layer = self.layers[i]\n",
    "            if layer.bias:\n",
    "                n_in += 1\n",
    "            layer.W = SIGMA_INIT * np.random.randn(layer.n, n_in)\n",
    "            n_in = layer.n\n",
    "        self.loss = loss\n",
    "        self.trainer = trainer\n",
    "        trainer.model = self\n",
    "        trainer.deltas = [None for i in xrange(len(self.layers))]\n",
    "    def forward(self, X):\n",
    "        buf = X.T\n",
    "        for i in xrange(self.nl):\n",
    "            layer = self.layers[i]\n",
    "            if layer.bias:\n",
    "                buf = np.append(buf, np.ones((1, buf.shape[1])), axis=0)\n",
    "            layer._in = buf.copy()\n",
    "            buf = np.dot(layer.W, buf)\n",
    "            layer._out = buf.copy()\n",
    "            buf = layer.act.f(buf)\n",
    "        return buf.T\n",
    "    def backward(self, Y_pred, Y_target):\n",
    "        assert self.loss\n",
    "        assert self.trainer\n",
    "        prev = None\n",
    "        for i in xrange(self.nl - 1, -1, -1):\n",
    "            layer = self.layers[i]\n",
    "            if not prev:\n",
    "                err = self.loss.f_deriv(Y_pred, Y_target).T\n",
    "            else:\n",
    "                err = np.dot(prev.W.T, prev.deltas)\n",
    "            # consider bias\n",
    "            if layer._out.shape[0] < err.shape[0]:\n",
    "                out = np.append(layer._out, np.ones((1, err.shape[1])), axis=0)\n",
    "            else:\n",
    "                out = layer._out.shape[0]\n",
    "            err *= layer.act.f_deriv(out)\n",
    "            layer.deltas = err.copy()\n",
    "            prev = layer\n",
    "    def _forward_backward(self, X, Y_target):\n",
    "        Y_pred = self.forward(X)\n",
    "        self.backward(Y_pred, Y_target)\n",
    "        self.trainer.update()\n",
    "    def train_epoch(self, X, Y, batch_size=1, shuffle=True):\n",
    "        if shuffle:\n",
    "            p = np.random.permutation(X.shape[0])\n",
    "            X = X[p]\n",
    "            Y = Y[p]\n",
    "        if batch_size < 1:\n",
    "            batch_size = X.shape[0]\n",
    "        for i in xrange(0, X.shape[0], batch_size):\n",
    "            self._forward_backward(X[i:min(i+batch_size,X.shape[0])], Y[i:min(i+batch_size,X.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000L, 1L)\n",
      "[ 0.46955094  0.05291806] [ 0.27297385] [ 0.]\n",
      "Epoch     1 -   Train Loss: 346.816   Test  Loss: 90.016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:32: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     2 -   Train Loss: 347.348   Test  Loss: 90.792\n",
      "Epoch     3 -   Train Loss: 346.398   Test  Loss: 90.261\n",
      "Epoch     4 -   Train Loss: 346.618   Test  Loss: 90.028\n",
      "Epoch     5 -   Train Loss: 346.549   Test  Loss: 90.383\n",
      "Epoch     6 -   Train Loss: 349.428   Test  Loss: 90.261\n",
      "Epoch     7 -   Train Loss: 346.548   Test  Loss: 90.037\n",
      "Epoch     8 -   Train Loss: 354.430   Test  Loss: 93.265\n",
      "Epoch     9 -   Train Loss: 346.531   Test  Loss: 90.373\n",
      "Epoch    10 -   Train Loss: 346.729   Test  Loss: 90.017\n",
      "Epoch    11 -   Train Loss: 346.391   Test  Loss: 90.261\n",
      "Epoch    12 -   Train Loss: 346.890   Test  Loss: 90.011\n",
      "Epoch    13 -   Train Loss: 346.352   Test  Loss: 90.218\n",
      "Epoch    14 -   Train Loss: 346.330   Test  Loss: 90.134\n",
      "Epoch    15 -   Train Loss: 346.668   Test  Loss: 90.016\n",
      "Epoch    16 -   Train Loss: 346.814   Test  Loss: 90.545\n",
      "Epoch    17 -   Train Loss: 348.636   Test  Loss: 90.151\n",
      "Epoch    18 -   Train Loss: 355.739   Test  Loss: 93.678\n",
      "Epoch    19 -   Train Loss: 346.785   Test  Loss: 90.004\n",
      "Epoch    20 -   Train Loss: 346.807   Test  Loss: 90.001\n",
      "Epoch    21 -   Train Loss: 346.282   Test  Loss: 90.140\n",
      "Epoch    22 -   Train Loss: 347.950   Test  Loss: 91.054\n",
      "Epoch    23 -   Train Loss: 347.309   Test  Loss: 90.790\n",
      "Epoch    24 -   Train Loss: 348.974   Test  Loss: 91.446\n",
      "Epoch    25 -   Train Loss: 348.739   Test  Loss: 91.361\n",
      "Epoch    26 -   Train Loss: 347.427   Test  Loss: 90.850\n",
      "Epoch    27 -   Train Loss: 346.798   Test  Loss: 89.974\n",
      "Epoch    28 -   Train Loss: 346.177   Test  Loss: 90.165\n",
      "Epoch    29 -   Train Loss: 346.514   Test  Loss: 90.436\n",
      "Epoch    30 -   Train Loss: 347.244   Test  Loss: 89.975\n",
      "Epoch    31 -   Train Loss: 347.107   Test  Loss: 89.959\n",
      "Epoch    32 -   Train Loss: 348.781   Test  Loss: 90.134\n",
      "Epoch    33 -   Train Loss: 346.030   Test  Loss: 90.170\n",
      "Epoch    34 -   Train Loss: 346.682   Test  Loss: 90.574\n",
      "Epoch    35 -   Train Loss: 348.180   Test  Loss: 90.035\n",
      "Epoch    36 -   Train Loss: 346.265   Test  Loss: 90.401\n",
      "Epoch    37 -   Train Loss: 345.959   Test  Loss: 89.866\n",
      "Epoch    38 -   Train Loss: 346.140   Test  Loss: 90.382\n",
      "Epoch    39 -   Train Loss: 348.644   Test  Loss: 90.055\n",
      "Epoch    40 -   Train Loss: 347.931   Test  Loss: 91.158\n",
      "Epoch    41 -   Train Loss: 346.636   Test  Loss: 90.676\n",
      "Epoch    42 -   Train Loss: 347.772   Test  Loss: 89.885\n",
      "Epoch    43 -   Train Loss: 345.041   Test  Loss: 89.674\n",
      "Epoch    44 -   Train Loss: 344.749   Test  Loss: 89.862\n",
      "Epoch    45 -   Train Loss: 344.611   Test  Loss: 89.546\n",
      "Epoch    46 -   Train Loss: 344.135   Test  Loss: 89.547\n",
      "Epoch    47 -   Train Loss: 343.797   Test  Loss: 89.453\n",
      "Epoch    48 -   Train Loss: 346.345   Test  Loss: 89.492\n",
      "Epoch    49 -   Train Loss: 342.937   Test  Loss: 89.217\n",
      "Epoch    50 -   Train Loss: 342.342   Test  Loss: 89.132\n",
      "Epoch    51 -   Train Loss: 342.114   Test  Loss: 88.827\n",
      "Epoch    52 -   Train Loss: 341.047   Test  Loss: 88.957\n",
      "Epoch    53 -   Train Loss: 339.926   Test  Loss: 88.548\n",
      "Epoch    54 -   Train Loss: 338.750   Test  Loss: 88.230\n",
      "Epoch    55 -   Train Loss: 337.320   Test  Loss: 87.879\n",
      "Epoch    56 -   Train Loss: 337.532   Test  Loss: 88.412\n",
      "Epoch    57 -   Train Loss: 333.297   Test  Loss: 86.864\n",
      "Epoch    58 -   Train Loss: 333.598   Test  Loss: 86.225\n",
      "Epoch    59 -   Train Loss: 326.606   Test  Loss: 85.108\n",
      "Epoch    60 -   Train Loss: 322.671   Test  Loss: 83.703\n",
      "Epoch    61 -   Train Loss: 314.854   Test  Loss: 81.861\n",
      "Epoch    62 -   Train Loss: 304.432   Test  Loss: 79.441\n",
      "Epoch    63 -   Train Loss: 289.559   Test  Loss: 75.494\n",
      "Epoch    64 -   Train Loss: 267.278   Test  Loss: 69.813\n",
      "Epoch    65 -   Train Loss: 235.832   Test  Loss: 61.293\n",
      "Epoch    66 -   Train Loss: 191.410   Test  Loss: 49.652\n",
      "Epoch    67 -   Train Loss: 132.876   Test  Loss: 34.535\n",
      "Epoch    68 -   Train Loss: 75.658   Test  Loss: 19.677\n",
      "Epoch    69 -   Train Loss: 36.943   Test  Loss: 9.560\n",
      "Epoch    70 -   Train Loss: 12.983   Test  Loss: 3.389\n",
      "Epoch    71 -   Train Loss: 4.476   Test  Loss: 1.186\n",
      "Epoch    72 -   Train Loss: 2.480   Test  Loss: 0.665\n",
      "Epoch    73 -   Train Loss: 2.529   Test  Loss: 0.661\n",
      "Epoch    74 -   Train Loss: 2.616   Test  Loss: 0.683\n",
      "Epoch    75 -   Train Loss: 2.581   Test  Loss: 0.671\n",
      "Epoch    76 -   Train Loss: 2.422   Test  Loss: 0.627\n",
      "Epoch    77 -   Train Loss: 2.266   Test  Loss: 0.587\n",
      "Epoch    78 -   Train Loss: 2.046   Test  Loss: 0.530\n",
      "Epoch    79 -   Train Loss: 1.843   Test  Loss: 0.476\n",
      "Epoch    80 -   Train Loss: 1.673   Test  Loss: 0.432\n",
      "Epoch    81 -   Train Loss: 1.524   Test  Loss: 0.394\n",
      "Epoch    82 -   Train Loss: 1.398   Test  Loss: 0.361\n",
      "Epoch    83 -   Train Loss: 1.288   Test  Loss: 0.333\n",
      "Epoch    84 -   Train Loss: 1.189   Test  Loss: 0.307\n",
      "Epoch    85 -   Train Loss: 1.120   Test  Loss: 0.290\n",
      "Epoch    86 -   Train Loss: 1.036   Test  Loss: 0.267\n",
      "Epoch    87 -   Train Loss: 0.987   Test  Loss: 0.256\n",
      "Epoch    88 -   Train Loss: 0.918   Test  Loss: 0.238\n",
      "Epoch    89 -   Train Loss: 0.868   Test  Loss: 0.225\n",
      "Epoch    90 -   Train Loss: 0.828   Test  Loss: 0.215\n",
      "Epoch    91 -   Train Loss: 0.791   Test  Loss: 0.205\n",
      "Epoch    92 -   Train Loss: 0.754   Test  Loss: 0.196\n",
      "Epoch    93 -   Train Loss: 0.727   Test  Loss: 0.189\n",
      "Epoch    94 -   Train Loss: 0.717   Test  Loss: 0.186\n",
      "Epoch    95 -   Train Loss: 0.696   Test  Loss: 0.181\n",
      "Epoch    96 -   Train Loss: 0.662   Test  Loss: 0.173\n",
      "Epoch    97 -   Train Loss: 0.654   Test  Loss: 0.171\n",
      "Epoch    98 -   Train Loss: 0.632   Test  Loss: 0.165\n",
      "Epoch    99 -   Train Loss: 0.632   Test  Loss: 0.165\n",
      "Epoch   100 -   Train Loss: 0.609   Test  Loss: 0.160\n",
      "[[ 0.37147238  0.37124712]\n",
      " [ 0.44598616  0.32773611]\n",
      " [ 0.37837422  0.29096734]\n",
      " [ 0.31852708  0.10414963]\n",
      " [ 0.29191582  0.48600938]]\n",
      "[[ 0.55163225]\n",
      " [ 0.59864615]\n",
      " [ 0.44801812]\n",
      " [ 0.1786556 ]\n",
      " [ 0.60516761]]\n",
      "[[ 0.56212436]\n",
      " [ 0.60980754]\n",
      " [ 0.45277315]\n",
      " [ 0.1688439 ]\n",
      " [ 0.61667026]]\n",
      "[[ 0.46955094  0.05291806]\n",
      " [ 0.30149947  0.14115633]\n",
      " [ 0.35878371  0.18207351]\n",
      " [ 0.42161186  0.22756778]\n",
      " [ 0.17876483  0.3561624 ]]\n",
      "[[ 0.27297385]\n",
      " [ 0.19594416]\n",
      " [ 0.29252653]\n",
      " [ 0.4214342 ]\n",
      " [ 0.28614715]]\n",
      "[[ 0.26416898]\n",
      " [ 0.18566143]\n",
      " [ 0.28498917]\n",
      " [ 0.42404372]\n",
      " [ 0.27842422]]\n"
     ]
    }
   ],
   "source": [
    "# MLP - Regression\n",
    "\n",
    "N_IN = 2\n",
    "N_HIDDEN = 4\n",
    "N_OUT = 1\n",
    "N_SAMPLES = 10000\n",
    "LEARNING_RATE = 0.1\n",
    "MOMENTUM = 0.9\n",
    "N_EPOCHS = 100\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "X = np.random.rand(N_SAMPLES, N_IN) * 0.5\n",
    "Y = np.zeros((N_SAMPLES, N_OUT))\n",
    "Y = np.sum(X, axis=1)**2\n",
    "Y = Y[:,np.newaxis]\n",
    "print Y.shape\n",
    "\n",
    "print X[0],Y[0],Y[0]-(X[0,0]+X[0,1])**2\n",
    "\n",
    "thresh = int(0.8 * N_SAMPLES)\n",
    "X_train = X[:thresh]\n",
    "Y_train = Y[:thresh]\n",
    "X_test = X[thresh:]\n",
    "Y_test = Y[thresh:]\n",
    "\n",
    "logistic = Activation(lambda X: sigmoid(X), lambda Y: sigmoid(Y) * (1. - sigmoid(Y)))\n",
    "identity = Activation(lambda X: X, lambda Y: Y)\n",
    "\n",
    "mse = Loss(\n",
    "    lambda Y_pred, Y_target: np.sum((Y_pred - Y_target)**2, axis=1), \n",
    "    lambda Y_pred, Y_target: Y_pred - Y_target\n",
    ")\n",
    "\n",
    "sgd = SGD(LEARNING_RATE, MOMENTUM)\n",
    "\n",
    "hidden = Layer(N_HIDDEN, logistic, bias=True)\n",
    "hidden2 = Layer(N_HIDDEN/2, logistic, bias=True)\n",
    "output = Layer(N_OUT, identity, bias=False)\n",
    "\n",
    "model = Model(N_IN)\n",
    "model.add(hidden)\n",
    "model.add(hidden2)\n",
    "model.add(output)\n",
    "model.prepare(loss=mse, trainer=sgd)\n",
    "\n",
    "for i in range(N_EPOCHS):\n",
    "    model.train_epoch(X_train, Y_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    Y_train_pred = model.forward(X_train)\n",
    "    Y_test_pred = model.forward(X_test)\n",
    "    if not (i+1) % 1:\n",
    "        print \"Epoch %5i\" % (i+1), \"-\", \"  Train Loss: %5.3f\" % np.sum(mse.f(Y_train_pred, Y_train), axis=0), \\\n",
    "            \"  Test  Loss: %5.3f\" % np.sum(mse.f(Y_test_pred, Y_test), axis=0)\n",
    "        \n",
    "print X_test[:5]\n",
    "print Y_test[:5]\n",
    "Y_pred = model.forward(X_test[:5])\n",
    "print Y_pred\n",
    "print X_train[:5]\n",
    "print Y_train[:5]\n",
    "Y_pred = model.forward(X_train[:5])\n",
    "print Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: logistic layer, softmax and classification examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: RNN and BPTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#some other things: droput, regularization, batch normalization, weight decay, momentum, stopping criteria for trainer, \n",
    "# automatic minibatch sizing, ...\n",
    "\n",
    "#out of scope: computational graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
