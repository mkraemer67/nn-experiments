{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just various neural networks and associated techniques in a modular fashion utilizing matrix notation.\n",
    "# Nothing groundbreaking; just for exercising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SIGMA_INIT = 0.001\n",
    "\n",
    "class Activation(object):\n",
    "    def __init__(self, f, f_deriv):\n",
    "        self.f = f\n",
    "        self.f_deriv = f_deriv\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self, n, activation, bias):\n",
    "        self.n = n\n",
    "        self.W = None\n",
    "        self.act = activation\n",
    "        self.bias = bias\n",
    "\n",
    "class Loss(object):\n",
    "    def __init__(self, loss_function, loss_function_deriv):\n",
    "        self.f = loss_function\n",
    "        self.f_deriv = loss_function_deriv\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, learning_rate):\n",
    "        self.lr = learning_rate\n",
    "    def update(self):\n",
    "        raise Exception(\"Use implementing subclass.\")\n",
    "\n",
    "class SGD(Trainer):\n",
    "    def __init__(self, learning_rate, momentum):\n",
    "        super(SGD, self).__init__(learning_rate)\n",
    "        self.momentum = momentum\n",
    "    def update(self):\n",
    "        for i, layer in enumerate(self.model.layers):\n",
    "            if self.deltas[i] != None:\n",
    "                layer.W -= self.momentum * self.deltas[i]\n",
    "            # consider bias\n",
    "            imax = layer.W.shape[0]\n",
    "            delta = self.lr * np.dot(layer.deltas[:imax], layer._in.T)\n",
    "            delta /= layer.deltas.shape[1] \n",
    "            layer.W -= delta\n",
    "            self.deltas[i] = delta\n",
    "        \n",
    "# TODO: Change to in place operations for optimization where possible.\n",
    "class Model(object):\n",
    "    def __init__(self, n_in):\n",
    "        self.n_in = n_in\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.trainer = None\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    def prepare(self, loss, trainer):\n",
    "        self.nl = len(self.layers)\n",
    "        assert self.nl > 0\n",
    "        n_in = self.n_in\n",
    "        for i in xrange(self.nl):\n",
    "            layer = self.layers[i]\n",
    "            if layer.bias:\n",
    "                n_in += 1\n",
    "            layer.W = SIGMA_INIT * np.random.randn(layer.n, n_in)\n",
    "            n_in = layer.n\n",
    "        self.loss = loss\n",
    "        self.trainer = trainer\n",
    "        trainer.model = self\n",
    "        trainer.deltas = [None for i in xrange(len(self.layers))]\n",
    "    def forward(self, X):\n",
    "        buf = X.T\n",
    "        for i in xrange(self.nl):\n",
    "            layer = self.layers[i]\n",
    "            if layer.bias:\n",
    "                buf = np.append(buf, np.ones((1, buf.shape[1])), axis=0)\n",
    "            layer._in = buf.copy()\n",
    "            buf = np.dot(layer.W, buf)\n",
    "            layer._out = buf.copy()\n",
    "            buf = layer.act.f(buf)\n",
    "        return buf.T\n",
    "    def backward(self, Y_pred, Y_target):\n",
    "        assert self.loss\n",
    "        assert self.trainer\n",
    "        prev = None\n",
    "        for i in xrange(self.nl - 1, -1, -1):\n",
    "            layer = self.layers[i]\n",
    "            if not prev:\n",
    "                err = self.loss.f_deriv(Y_pred, Y_target).T\n",
    "            else:\n",
    "                err = np.dot(prev.W.T, prev.deltas)\n",
    "            # consider bias\n",
    "            if layer._out.shape[0] < err.shape[0]:\n",
    "                out = np.append(layer._out, np.ones((1, err.shape[1])), axis=0)\n",
    "            else:\n",
    "                out = layer._out\n",
    "            err *= layer.act.f_deriv(out)\n",
    "            err = np.clip(err, -500, 500)\n",
    "            layer.deltas = err.copy()\n",
    "            prev = layer\n",
    "    def _forward_backward(self, X, Y_target):\n",
    "        Y_pred = self.forward(X)\n",
    "        self.backward(Y_pred, Y_target)\n",
    "        self.trainer.update()\n",
    "    def train_epoch(self, X, Y, batch_size=1, shuffle=True):\n",
    "        if shuffle:\n",
    "            p = np.random.permutation(X.shape[0])\n",
    "            X = X[p]\n",
    "            Y = Y[p]\n",
    "        if batch_size < 1:\n",
    "            batch_size = X.shape[0]\n",
    "        for i in xrange(0, X.shape[0], batch_size):\n",
    "            self._forward_backward(X[i:min(i+batch_size,X.shape[0])], Y[i:min(i+batch_size,X.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000L, 1L)\n",
      "[ 0.46607615  0.39754986] [ 0.74584989] [ 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:32: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    10 - Train Loss: 70.666 Test  Loss: 15.889\n",
      "Epoch    20 - Train Loss: 70.993 Test  Loss: 15.957\n",
      "Epoch    30 - Train Loss: 70.663 Test  Loss: 15.879\n",
      "Epoch    40 - Train Loss: 70.506 Test  Loss: 15.845\n",
      "Epoch    50 - Train Loss: 70.534 Test  Loss: 15.858\n",
      "Epoch    60 - Train Loss: 70.309 Test  Loss: 15.800\n",
      "Epoch    70 - Train Loss: 69.987 Test  Loss: 15.736\n",
      "Epoch    80 - Train Loss: 68.875 Test  Loss: 15.483\n",
      "Epoch    90 - Train Loss: 65.688 Test  Loss: 14.776\n",
      "Epoch   100 - Train Loss: 56.014 Test  Loss: 12.635\n",
      "Epoch   110 - Train Loss: 35.479 Test  Loss: 8.267\n",
      "Epoch   120 - Train Loss: 10.098 Test  Loss: 2.389\n",
      "Epoch   130 - Train Loss: 2.819 Test  Loss: 0.653\n",
      "Epoch   140 - Train Loss: 1.269 Test  Loss: 0.279\n",
      "Epoch   150 - Train Loss: 0.880 Test  Loss: 0.187\n",
      "Epoch   160 - Train Loss: 0.765 Test  Loss: 0.160\n",
      "Epoch   170 - Train Loss: 0.705 Test  Loss: 0.147\n",
      "Epoch   180 - Train Loss: 0.683 Test  Loss: 0.142\n",
      "Epoch   190 - Train Loss: 0.670 Test  Loss: 0.140\n",
      "Epoch   200 - Train Loss: 0.643 Test  Loss: 0.134\n",
      "Epoch   210 - Train Loss: 0.626 Test  Loss: 0.130\n",
      "Epoch   220 - Train Loss: 0.612 Test  Loss: 0.127\n",
      "Epoch   230 - Train Loss: 0.601 Test  Loss: 0.124\n",
      "Epoch   240 - Train Loss: 0.585 Test  Loss: 0.121\n",
      "Epoch   250 - Train Loss: 0.568 Test  Loss: 0.117\n",
      "Epoch   260 - Train Loss: 0.555 Test  Loss: 0.115\n",
      "Epoch   270 - Train Loss: 0.543 Test  Loss: 0.112\n",
      "Epoch   280 - Train Loss: 0.531 Test  Loss: 0.109\n",
      "Epoch   290 - Train Loss: 0.522 Test  Loss: 0.108\n",
      "Epoch   300 - Train Loss: 0.509 Test  Loss: 0.105\n",
      "[[ 0.33017932  0.10601213]\n",
      " [ 0.20585817  0.30130635]\n",
      " [ 0.40879558  0.00048482]\n",
      " [ 0.13491069  0.25447976]\n",
      " [ 0.34795987  0.00489321]]\n",
      "[[ 0.19026298]\n",
      " [ 0.25721585]\n",
      " [ 0.16751044]\n",
      " [ 0.15162492]\n",
      " [ 0.12450529]]\n",
      "[[ 0.19567597]\n",
      " [ 0.25526397]\n",
      " [ 0.17633012]\n",
      " [ 0.16369944]\n",
      " [ 0.14149953]]\n",
      "[[ 0.46607615  0.39754986]\n",
      " [ 0.07040523  0.31275902]\n",
      " [ 0.03945223  0.30702401]\n",
      " [ 0.19582596  0.31092975]\n",
      " [ 0.23823065  0.108423  ]]\n",
      "[[ 0.74584989]\n",
      " [ 0.14681484]\n",
      " [ 0.12004578]\n",
      " [ 0.25680134]\n",
      " [ 0.12016876]]\n",
      "[[ 0.7505005 ]\n",
      " [ 0.1598782 ]\n",
      " [ 0.13843041]\n",
      " [ 0.25490973]\n",
      " [ 0.13823297]]\n"
     ]
    }
   ],
   "source": [
    "# MLP - Regression\n",
    "\n",
    "N_IN = 2\n",
    "N_HIDDEN = 4\n",
    "N_OUT = 1\n",
    "N_SAMPLES = 2000\n",
    "LEARNING_RATE = 0.05\n",
    "MOMENTUM = 0.9\n",
    "N_EPOCHS = 300\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "X = np.random.rand(N_SAMPLES, N_IN) * 0.5\n",
    "Y = np.zeros((N_SAMPLES, N_OUT))\n",
    "Y = np.sum(X, axis=1)**2\n",
    "Y = Y[:,np.newaxis]\n",
    "print Y.shape\n",
    "\n",
    "print X[0],Y[0],Y[0]-(X[0,0]+X[0,1])**2\n",
    "\n",
    "thresh = int(0.8 * N_SAMPLES)\n",
    "X_train = X[:thresh]\n",
    "Y_train = Y[:thresh]\n",
    "X_test = X[thresh:]\n",
    "Y_test = Y[thresh:]\n",
    "\n",
    "def sigmoid(x):\n",
    "    val = 1. / (1. + np.exp(-x))\n",
    "    return np.clip(val, -500, 500)\n",
    "    \n",
    "logistic = Activation(lambda X: sigmoid(X), lambda Y: sigmoid(Y) * (1. - sigmoid(Y)))\n",
    "identity = Activation(lambda X: X, lambda Y: Y)\n",
    "\n",
    "mse = Loss(\n",
    "    lambda Y_pred, Y_target: np.sum((Y_pred - Y_target)**2, axis=1), \n",
    "    lambda Y_pred, Y_target: Y_pred - Y_target\n",
    ")\n",
    "\n",
    "sgd = SGD(LEARNING_RATE, MOMENTUM)\n",
    "\n",
    "hidden = Layer(N_HIDDEN, logistic, bias=True)\n",
    "hidden2 = Layer(N_HIDDEN/2, logistic, bias=True)\n",
    "output = Layer(N_OUT, identity, bias=False)\n",
    "\n",
    "model = Model(N_IN)\n",
    "model.add(hidden)\n",
    "model.add(hidden2)\n",
    "model.add(output)\n",
    "model.prepare(loss=mse, trainer=sgd)\n",
    "\n",
    "for i in range(N_EPOCHS):\n",
    "    model.train_epoch(X_train, Y_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    Y_train_pred = model.forward(X_train)\n",
    "    Y_test_pred = model.forward(X_test)\n",
    "    if not (i+1) % 10:\n",
    "        print \"Epoch %5i\" % (i+1), \"-\", \"Train Loss: %5.3f\" % np.sum(mse.f(Y_train_pred, Y_train), axis=0), \\\n",
    "            \"Test  Loss: %5.3f\" % np.sum(mse.f(Y_test_pred, Y_test), axis=0)\n",
    "        \n",
    "print X_test[:5]\n",
    "print Y_test[:5]\n",
    "Y_pred = model.forward(X_test[:5])\n",
    "print Y_pred\n",
    "print X_train[:5]\n",
    "print Y_train[:5]\n",
    "Y_pred = model.forward(X_train[:5])\n",
    "print Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MLP - Classification\n",
    "\n",
    "\n",
    "#TODO: logistic layer, softmax and classification examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: RNN and BPTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#some other things: droput, regularization, batch normalization, weight decay, momentum, stopping criteria for trainer, \n",
    "# automatic minibatch sizing, ...\n",
    "\n",
    "#out of scope: computational graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
